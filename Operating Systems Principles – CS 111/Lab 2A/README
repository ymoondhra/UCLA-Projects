#NAME: Yash Moondhra
#EMAIL: ymoondhra@gmail.com
#ID: 123456789

QUESTIONS
2.1.1:
Why does it take many iterations before errors are seen?
Why does a significantly smaller number of iterations so seldom fail?

    The chance of two threads colliding in attempting to access the same shared data is very small. When there are a low amount of iterations, the total times that the shared memory is accessed is small (threads*iterations). However, with a higher volume of iterations and with a decent amount of threads, there is a reasonable chance that multiple threads will collide on access.


2.1.2:
Why are the --yield runs so much slower?
    The yield runs are so much slower because we are stopping the thread while it is in the add function in order to allow another thread to run. This is an expensive operation, as it requires a context switch.
Where is the additional time going?
      The additional time is going to the context switch: saving the current data and state and swapping it out for the data of the new thread that will be running next.
Is it possible to get valid per-operation timings if we are using the --yield option?
   No, it is not possible. The time cost of the context switches are variable, depending on the current thread.

 
2.1.3:
Why does the average cost per operation drop with increasing iterations?
    Thread creation (pthread_create) causes significant overhead. As we increase the amount of iterations, the program spends a smaller percentage of the total time on the pthread_create command, resulting in more time spent executing other instructions.
If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the "correct" cost is)?
   As discussed in the previous paragraph, pthread_create causes significant overhead, and the more iterations we have, the more time is spent executing other code (e.g. the add function). Therefore, if we significantly increase the number of iterations of the threads, the time creating the threads will become insignificant, and will approach the actual cost of the operation.   


2.1.4:
Why do all of the options perform similarly for low numbers of threads?
    The options perform similarly when we use a lower number of threads because there are a fewer race conditions, as we have less threads operating simultaneously. 
Why do the three protected operations slow down as the number of threads rises?
    As the number of threads rise, we increase the likelihood of collisions/race conditions. Whenever there is a race condition, the threads must wait until the other thread finishes, which causes overhead.


2.2.1:
Compare the variation in time per mutex-protected operation vs the number of threads in Part-1 (adds) and Part-2 (sorted lists).
Comment on the general shapes of the curves, and explain why they have this shape.
Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.
	
	The cost of inserting nodes into a list, checking the list's length, and then deleting the list, is much more expensive that simply adding an integer to a shared variable. Therefore, the list operations should increase much faster than the add ones. The mutex lock causes the other waiting threads to sleep to avoid wasting CPU cycles. The total waiting time of threads per second increases exponentially as the number of threads increases.   


2.2.2:
Compare the variation in time per protected operation vs the number of threads for list operations protected by Mutex vs Spin locks. Comment on the general shapes of the curves, and explain why they have this shape.
Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.

	As we increase the number of threads, both types of synchronization have the pattern that the cost of the operation increases in a linear pattern, with the spin lock caving slightly lower than the mutex. However, the y-axis grows exponentially, so the cost per operation grows exponentially as well. The reason for this exponential growth is that more threads simultaneously running causes more contention. Contention is the phenomenon where two or more threads attempt to access the same piece of data at the same time. The frequency of this occurrence increases as we have more threads, causing the cost of the spin lock to increase. Mutex makes the waiting threads sleep. 


Description of Files:
* lab2_add.c: a C program that creates threads that each add 1 to a shared counter variable and then subtract -1 from that variable, for a certain number of iterations
* lab2_list.c: a C program that creates that creates threads which insert elements in sorted order to a SortedList object, get the length of the list, and then delete all the elements
* SortedList.h: the header file which defines the structure of the SortedList object, which represents a doubly linked list of nodes with c string keys
* SortedList.c: the implementations for four list functions: insert, length, lookup, and delete
* lab2_add.csv, lab2_list.csv: the comma-separated data for testing lab2_list.c and lab2_add.c
* Makefile: `make` creates the executables for lab2_list.c and lab2_add.c. `make tests` creates the data for the csv files described above. `make graphs` creates the graphs which depict the trends from the csv data
* the .png files are the graphs created by running the .gp files. The .gp files analyze the data from the .csv files. 

Sources:
* implementing pthreads: https://github.com/ymoondhra/UCLA-Projects/blob/master/Software%20Construction%20Laboratory%20%E2%80%93%20CS%2035L/Assignment%206/srt/main.c